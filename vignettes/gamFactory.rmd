---
title: "gamFactory: tool for building GAM models in mgcv"
author: "Matteo Fasiolo"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
    number_sections: true
vignette: >
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteIndexEntry{mgcViz_vignette}
  %\VignetteEncoding{UTF-8}
---

<style>
body {
text-align: justify}
</style>

```{r setup, include=FALSE}
library(knitr)
library(rgl)
opts_chunk$set(out.extra='style="display:block; margin: auto"', fig.align="center", tidy=FALSE)
knit_hooks$set(webgl = hook_webgl)
```


# Testing derivatives of response family

As an example I consider the Generalized Pareto Distribution. We first create the family:
```{r 1, message = F}
library(gamFactory)

n <- 1000
pars <- c(rnorm(1, 0, 1), 1e-2 + rexp(1, 1), rexp(1, 1))
obj <- createGPD( )$initialize(n, pars)
```
The parameters have been randomly simulated, and `n=1000` observations are simulated via `createGPD( )$initialize`. Now we compare exact (`EX`) and finite difference (`FD`) derivative of up to order 3 at the fixed parameters: 
```{r 2, message = F}
fdDeriv(obj = derFunWrapper(obj$derObj), 
        param = pars, 
        ord = 1:3)
```

Now we do a more systematic check, by simulating `np` vectors of parameters from `parSim` and checking the derivatives at each:
```{r 3, message = F}
der <- derivCheck(np = 100, 
                  parSim = function(n){ cbind(rnorm(n), 1 + rexp(n, 1), rexp(n, 1)) }, 
                  obj = obj,
                  ord = 1:3, 
                  trans = function(.x){
                    si <- sign(.x)
                    return( si * sqrt(abs(.x)) )
                  }, 
                  n = 100)
```
Plotting the difference between the analytic and finite-difference derivatives is also useful:
```{r 4, message = F}
par(mfrow = c(2, 2))
for(ii in 1:3) { plot(der[[ii]][ , 1] - der[[ii]][ , 2]) }
```

# Testing derivatives of a non-linear effect

Here we consider an effect of the type
$$
\eta = \sum_{j}\alpha_{j}(z_{i})x_{ij}
$$
where 
$$
\alpha_{1}(z)=\frac{1}{1+\sum_{j\neq1}\exp(\nu_{j})}, \;\;\;\;
\alpha_{k}(z)=\frac{\exp(\nu_{k})}{1+\sum_{j\neq1}\exp(\nu_{j})},\;\;\;\text{for}\;\;\;k=2,\dots,K,
$$
so that $\sum_{j}\alpha_{j}(z_{i})=1$. To create such an effect we do
```{r a1, message = F}
library(gamFactory)

K <- 5
alpha0 <- rnorm(K - 1) 
obj <- createStackEffect( )$initialize(d = K)
```
Then we check the first three derivatives at $alpha0$:
```{r a2, message = F}
fdDeriv(obj = derFunWrapper(obj$derObj, dropHessian = TRUE), 
        param = alpha0, 
        ord = 1:3)
```
Looks good. Now we do some more serious testing:
```{r a3, message = F}
der <- derivCheck(np = 100, 
                  parSim = function(n){ matrix(rnorm(n*(K-1)), n, K-1) }, 
                  obj = obj,
                  ord = 1:3, 
                  trans = function(.x){
                    si <- sign(.x)
                    return( si * sqrt(abs(.x)) )
                  }, 
                  d = K)
```
Different way of plotting:
```{r a4, message = F}
par(mfrow = c(2, 2))
for(ii in 1:3) { plot(der[[ii]][ , 1] - der[[ii]][ , 2]) }
```

# Testing derivatives of the stack effect (Christian)

This Section testes the function convertDerivStack2. 
Its difference with convertDerivStack is that functions are used to wrap all derivatives.
With respect to the previous version, other functions are introduced to avoid code repetition such as betaToPars and lEta.
In theory, this should avoid code repetition in the following testing part, even if it is still messy. 
However, stackFamilySHASH implements convertDerivStack, which does not return functions but the derivative vectors/matrices directly. 
The function convertDerivStack follows more or less the function gamlss.gH in mgcv.

What the following code does is:

* It creates parameters and model matrix for stack effects. 
* Then, given the parameters, it generates data from the SHASH distribution. 
* Then, it calculate derivatives of the log-likelihood with respect to the regression coefficients
* Then, it finds beta-hat and calculate the derivative beta-hat with respect to the smoothing parameter
# Finally, it calculates the derivative of the negative Hessian with respect to the smoothing parameter

```{r a5, message = F}
set.seed(0)
n <- 1e3
p <- 3
K <- 3
P <- p * (K - 1)

tau <- 1
eps <- 1
phi <- 1
sig <- exp(tau)
del <- exp(phi)
theta <- c(tau, eps, phi)

betaToPars <- function(beta) { # Transform beta to SHASH parameters
  p <- ncol(Z)
  K <- ncol(X)
  nu <- Z %*% matrix(beta, nrow = p, ncol = K - 1)
  a <- cbind(1, exp(nu)) / (1 + rowSums(exp(nu)))
  eta <- rowSums(a * X)
  mu <- eta
  pars <- cbind(mu, theta[1], theta[2], theta[3])
  pars
}

# Derivatives of log-likelihood wrt eta as functions of beta
lEta <- function(beta, deriv = 0) { 
  pars <- betaToPars(beta)
  objSH <- createSH(y = y)$derObj(param = pars, deriv = deriv)
  
  l <- objSH$d0(SUM = T)
  le <- lee <- leee <- NULL # default values
  if (deriv > 0) {
    le <- objSH$d1(SUM = F)[[1]]
    
    if (deriv > 1) {
      lee <- objSH$d2(SUM = F)[[1]]
      
      if (deriv > 2) {
        leee <- objSH$d3(SUM = F)[[1]]
      }
    }
  }

  return(list(l = l, le = le, lee = lee, leee = leee))
}

beta <- rnorm(P)
Z <- cbind(1, matrix(runif(n * (p - 1)), nrow = n))
X <- matrix(runif(n * K), nrow = n)
pars <- betaToPars(beta)
y <- pars[, 1] + (del * sig) * sinh((1/del) * asinh(qnorm(runif(n))) + (eps/del))

objSH <- createSH(y = y)$derObj(param = pars, deriv = 3)


#### FUNCTIONS FOR TESTING DERIVATIVES

# log-likelihood function
ll <- function(param) lEta(param, deriv = 0)$l
# gradient function
gr <- function(param) {
  lEtaList <- lEta(param, deriv = 1)
  convertDerivStack(X, Z, lEtaList)$derObj(param, deriv = 1)$d1()
}
# hessian function
hes <- function(param) {
  lEtaList <- lEta(param, deriv = 2)
  convertDerivStack(X, Z, lEtaList)$derObj(param, deriv = 2)$d2()
}
# function optimizing beta given the smoothing parameter
d3r <- function(r) {
  # penalty matrix with smoothing parameter lam
  S <- diag(rep(1, P))
  lam <- exp(r)
  # Inner step: optimize pen l wrt beta
  beta0 <- rep(0, P)
  
  optimization <- optim(rep(0, P), function(x) {
    - ll(x) + .5 * lam * t(x) %*% S %*% x
  }, gr = function(x) - gr(x) + lam * S %*% x,
  method = "BFGS", hessian = TRUE)
  
  betaHat <- optimization$par # beta hat as function of rho
  negHessian <- optimization$hessian
  invNegH <- solve(optimization$hessian)
  d1br <- - lam * invNegH %*% S %*% betaHat # derivative of beta hat wrt who
  list(betaHat = betaHat, d1br = d1br, negHessian = negHessian)
}
# derivative of the negative hessian wrt rho
d1h <- function(r) {
  
  d3rObj <- d3r(r)
  param <- d3rObj$betaHat
  d1b <- d3rObj$d1br
  lEtaList <- lEta(param, deriv = 3)
  d1H <- convertDerivStack(X, Z, lEtaList)$derObj(param, d1b = d1b, deriv = 3)$d1H()
  d1H + exp(r) * S # Valid only because of single smoothing parameter implemented
}

#### TEST OF DERIVATIVES

# gradient
GR <- data.frame(EX = gr(beta), 
                 FD = as.numeric(jacobian(func = ll, x = as.vector(beta))))
GR$DIFF <- apply(GR, 1, function(x) round(diff(x), 7))
with(GR, plot(EX, FD))
abline(0, 1, col = "red")
GR

# hessian
HEX <- data.frame(EX = as.numeric(hes(beta)),
                  FD = as.numeric(jacobian(func = gr, x = as.vector(beta))))
HEX$DIFF <- apply(HEX, 1, function(x) round(diff(x), 7))
with(HEX, plot(EX, FD))
abline(0, 1, col = "red")
HEX

# d beta / d rho
r <- 0
D1B <- data.frame(EX = d3r(r)$d1br, 
                  FD = jacobian(function(x) d3r(x)$betaHat, r))
D1B$DIFF <- apply(D1B, 1, function(x) round(diff(x), 2)) # less precise because of optim
with(D1B, plot(EX, FD))
abline(0, 1, col = "red")
D1B

# hessian wrt rho
# THERE ARE SOME PROBLEMS I THINK DUE TO IDENTIFIABILITY ISSUES
# TRUST ONLY IF NEGATIVE HESSIAN IS POSITIVE DEFINITE
D1H <- data.frame(EX = as.numeric(d1h(r)), 
                  FD = jacobian(function(x) d3r(x)$negHessian, r))
D1H$DIFF <- apply(D1H, 1, function(x) round(diff(x), 2))
with(D1H, plot(EX, FD))
abline(0, 1, col = "red")
D1H
```

# Example to show identifiabilty problems with the convex combination parametrization 

Consider the simplest possible case, where there is only one $\nu$, i.e. $K=2$, there are two alphas and therefore $K-1 = 1$. Moreover, $Z = 1$ so that $\beta = \nu$. We consider data $y$ from a Gaussian distribution, with $\sigma=1$ and identity link. Then, we have
$$\alpha_1 = \frac{1}{1 + \exp \nu}, \quad \alpha_2 = \frac{\exp \nu}{1 + \exp \nu}.$$
The problem is that using directly $\alpha$ we would not have the $(0,1)$ constraint. Then the log-likelihood can at zero and one does not go to minus infinity as, for example, happens for the parameters of a multinomial. Therefore, using the reparametrization that allows to treat the parameter $\nu$ as unconstrained, what happens is that the log-likelihood tends to a finite value as $\nu$ goes to minus or plus infinity. This creates problems in terms of convexity of the log-likelihood and convergence.

```{r}
library(ggplot2)
# set.seed(0)
n <- 1e2
p <- 1
K <- 2
P <- p * (K - 1)
beta <- rnorm(P)
beta <- rep(0, P)
Z <- cbind(1, matrix(runif(n * (p - 1)), nrow = n))
X <- matrix(rnorm(n * K), nrow = n, ncol = K, byrow = TRUE)
X <- cbind(rnorm(n, mean = 1), rnorm(n, mean = 1))
x1 <- rnorm(n, mean = 0)
x2 <- x1 + rnorm(n, mean = 0.1)
X <- cbind(x1, x2)
nu <- Z %*% matrix(beta, nrow = p, ncol = K - 1)
a <- cbind(1, exp(nu)) / (1 + rowSums(exp(nu)))
eta <- rowSums(a * X)
mu <- eta
y <- rnorm(n, mu)
le <- y - mu
lee <- rep(- n, n)
leee <- rep(0, n)

ret <- convertDerivStack(beta, theta, X, Z, le, lee, leee, d1b = 0, deriv = 1)

xx <- seq(from = - 10, to = 10, l = 200)
yy <- sapply(xx, function(b) {
  nu <- Z %*% matrix(b, nrow = p, ncol = K - 1)
  a <- cbind(1, exp(nu)) / (1 + rowSums(exp(nu)))
  eta <- rowSums(a * X)
  mu <- eta
  - .5 * sum((y - mu) ^ 2)
})

ggplot() +
  geom_line(aes(x = 1:n, y = x1), col = "blue") +
  geom_line(aes(x = 1:n, y = x2), col = "black") +
  geom_point(aes(x = 1:n, y = y), shape = "-", size = 6, col = "red")
plot(xx, yy, type = "l")
betaHatAnalytical <- log(- sum((y - X[, 1]) * (X[, 1] - X[, 2])) / sum((y - X[, 2]) * (X[, 1] - X[, 2])) )
abline(v = betaHatAnalytical)

## In the following, some code to see that estimates are not stable

beta0 <- rep(0, P)
# optimization <- optim(beta0, function(x) - l(x), gr = function(x) - gr(x),
# method = "BFGS", hessian = TRUE)
# betaHat <- optimization$par

l <- function(param) {
  nu <- Z %*% matrix(param, nrow = p, ncol = K - 1)
  a <- cbind(1, exp(nu)) / (1 + rowSums(exp(nu)))
  eta <- rowSums(a * X)
  mu <- eta
  - .5 * sum((y - mu) ^ 2)
}

optimization <- nlm(function(x) - l(x), beta0, hessian = TRUE, gradtol = 1e-10)
betaHat <- optimization$estimate


nuHat <- Z %*% matrix(betaHat, nrow = p, ncol = K - 1)
aHat <- cbind(1, exp(nuHat)) / (1 + rowSums(exp(nuHat)))

invNegH <- solve(optimization$hessian)
optimization$iterations
eigen(- hes(betaHat))$values %>% barplot
eigen(- hes(betaHat))$values
optimization$gradient
beta
betaHat
betaHatAnalytical
se <- diag(invNegH)
se

ret <- convertDerivStack(betaHatAnalytical, theta, X, Z, le, lee, leee, d1b = 0, deriv = 1)
ret$lb

ggplot() + 
  geom_errorbar((aes(1:P, ymin = betaHat - 1.96 * se, ymax = betaHat + 1.96 * se))) +
  geom_point(aes(1:P, beta), col = "red")

```

# PIRLS and shape constrained effects with linear approximation? 

Simple example to show that using the linear approximation 
of the unconstrained parameter beta tilde around the constrained parameter beta might allow using PIRLS to maximize directly the likelihood wrt beta when there are shape constrained effects.

In the simple example, the response variable is normal, with identity link and one smoothing parameter.

Maybe it is not useful, since we are focusing on the approach for general families.

response: normal
link: identity
penalty: one smoothing parameter

```{r a2, message = F}
set.seed(0)
r <- 0                              # rho = log(lambda)
l <- exp(r)                         # lambda the smoothing parameter
S <- diag(rep(1, p))                # penalty matrix
p <- 3                              # n. of parameters
n <- 1e4                            # n. of obs
b <- rnorm(p)                       # true beta (constrained)
bt <- c(b[1], exp(b[- 1]))          # beta tilde (unconstrained parameters)
btb <- c(1, bt[- 1])                # derivative of bt wrt b
X <- matrix(runif(n * p), nrow = n) # model matrix
mu <- X %*% bt
sig <- 1
y <- rnorm(n, mean = mu, sd = sig)  # response
b0 <- rep(- 1e4, p)                 # starting value

bh <- b0
G <- 10
niter <- 0
while(sum(G^2) > 1e-10) {
  bth <- c(bh[1], exp(bh[- 1]))
  muh <- X %*% bth
  btb <- c(1, exp(bh [- 1]))
  Xt <- t(t(X) * btb)
  G <- t(Xt) %*% (y - muh) - S %*% bh
  H <- - t(Xt) %*% Xt - S
  bh <- bh - solve(H) %*% G
  sum(G^2)
  bh
  niter <- niter + 1
}
bh
niter
```

# First family implemented that uses mgcv

The function stackFamilySHASH creates a new family that can be used in mgcv. The family allows using a model with stacking effect and a response variable with SHASH distribution.

The function estimateStackSHASH uses mgcv to estimate a model with the stackFamilySHASH.

Both functions are in the file stackFamilySHASH.R.

CURRENT PROBLEMS:

* Should pass X as family argument rather than attribute of y
* Identifiability!!! Even with increasing n and small p, estimates do not necessarily converge to true beta (see previous sections)
* Moreover, with the identifiability problem, as the estimates diverge, exp(nu) gives infinity and I the code creates problems.

```{r}
set.seed(0)
n <- 1e2
p <- 3
K <- 3
P <- p * (K - 1)
beta <- rnorm(P)
Z <- cbind(1, matrix(runif(n * (p - 1)), nrow = n))
# Z <- cbind(1, matrix(runif(n * p), nrow = n))
# Z <- scale(Z, scale = FALSE)
# Z <- Z[, - 1, drop = FALSE]

# X <- matrix(1, nrow = n, ncol = K)

X <- matrix(runif(n * K), nrow = n)

nu <- Z %*% matrix(beta, nrow = p, ncol = K - 1)
a <- cbind(1, exp(nu)) / (1 + rowSums(exp(nu)))
eta <- rowSums(a * X)
mu <- eta
tau <- 1
eps <- 1
phi <- 1
sig <- exp(tau)
del <- exp(phi)
y <- mu + (del * sig) * sinh((1/del) * asinh(qnorm(runif(n))) + (eps/del))

Z <- matrix(Z, nrow = n, ncol = p * (K - 1))
x <- Z
attr(y, "X") <- X
llf <- stackFamilySHASH()$ll
beta0 <- rep(1, P)
S <- diag(rep(1, P))
lam <- 0
optimization <- optim(beta0, function(b) {
  - llf(y, x, b)$l 
    # .5 * lam * b %*% S %*% b
}, gr = function(b) - llf(y, x, b)$lb, 
  # lam * S %*% b,
method = "BFGS", hessian = TRUE)
plot(optimization$par, beta, xlim = c(- 3, 3), ylim = c(- 3, 3))
abline(0,1, col = "red")
eigen(optimization$hessian)$values %>% barplot

nuHat <- matrix(Z, nrow = n, ncol = p) %*% matrix(optimization$par, nrow = p, ncol = K - 1)
aHat <- cbind(1, exp(nuHat)) / (1 + rowSums(exp(nuHat)))
etaHat <- rowSums(aHat * X)
muHat <- etaHat
plot(cbind(mu, muHat)[sample(1:n, 10^4), ], cex = 0.1)
# plot(cbind(mu, muHat), cex = 0.1)
abline(0,1, col = "red")

# debugonce(estimateStackSHASH)
mod <- estimateStackSHASH(y, X, Z)
cbind(mod$coefficients, beta)
summary(mod)
```



## Test of derivatives with new stack effects (only non-negative constraints)

```{r}
set.seed(0)
n <- 1e3
p <- 3
K <- 3
P <- p * K

tau <- 1
eps <- 1
phi <- 1
sig <- exp(tau)
del <- exp(phi)
theta <- c(tau, eps, phi)

betaToPars <- function(beta) { # Transform beta to SHASH parameters
  p <- ncol(Z)
  K <- ncol(X)
  nu <- Z %*% matrix(beta, nrow = p, ncol = K)
  a <- exp(nu)
  eta <- rowSums(a * X)
  mu <- eta
  pars <- cbind(mu, theta[1], theta[2], theta[3])
  pars
}

# Derivatives of log-likelihood wrt eta as functions of beta
lEta <- function(beta, deriv = 0) { 
  pars <- betaToPars(beta)
  objSH <- createSH(y = y)$derObj(param = pars, deriv = deriv)
  
  l <- objSH$d0(SUM = T)
  le <- lee <- leee <- NULL # default values
  if (deriv > 0) {
    le <- objSH$d1(SUM = F)[[1]]
    
    if (deriv > 1) {
      lee <- objSH$d2(SUM = F)[[1]]
      
      if (deriv > 2) {
        leee <- objSH$d3(SUM = F)[[1]]
      }
    }
  }
  
  return(list(l = l, le = le, lee = lee, leee = leee))
}

beta <- rnorm(P)
Z <- cbind(1, matrix(runif(n * (p - 1)), nrow = n))
X <- matrix(runif(n * K), nrow = n)
pars <- betaToPars(beta)
y <- pars[, 1] + (del * sig) * sinh((1/del) * asinh(qnorm(runif(n))) + (eps/del))

objSH <- createSH(y = y)$derObj(param = pars, deriv = 3)

l <- objSH$d0()
le <- objSH$d1(SUM = FALSE)[[1]]
lee <- objSH$d2(SUM = FALSE)[[1]]
leee <- objSH$d3(SUM = FALSE)[[1]]

#### FUNCTIONS FOR TESTING DERIVATIVES

# log-likelihood function
ll <- function(param) lEta(param, deriv = 0)$l
# gradient function
gr <- function(param) {
  lEtaList <- lEta(param, deriv = 1)
  le <- lEtaList$le
  convertDerivStackNonNegative(param, theta, X, Z, le, lee, leee)$lb
}
# hessian function
hes <- function(param) {
  lEtaList <- lEta(param, deriv = 2)
  le <- lEtaList$le
  lee <- lEtaList$lee
  convertDerivStackNonNegative(param, theta, X, Z, le, lee, leee)$lbb
}
# function optimizing beta given the smoothing parameter
d3r <- function(r) {
  # penalty matrix with smoothing parameter lam
  S <- diag(rep(1, P))
  lam <- exp(r)
  # Inner step: optimize pen l wrt beta
  beta0 <- rep(0, P)
  
  optimization <- optim(rep(0, P), function(x) {
    - ll(x) + .5 * lam * t(x) %*% S %*% x
  }, gr = function(x) - gr(x) + lam * S %*% x,
  method = "BFGS", hessian = TRUE)
  
  betaHat <- optimization$par # beta hat as function of rho
  negHessian <- optimization$hessian
  invNegH <- solve(optimization$hessian)
  d1br <- - lam * invNegH %*% S %*% betaHat # derivative of beta hat wrt who
  list(betaHat = betaHat, d1br = d1br, negHessian = negHessian)
}
# derivative of the negative hessian wrt rho
d1h <- function(r) {
  
  d3rObj <- d3r(r)
  param <- d3rObj$betaHat
  d1b <- d3rObj$d1br
  lEtaList <- lEta(param, deriv = 3)
  le <- lEtaList$le
  lee <- lEtaList$lee
  leee <- lEtaList$leee
  d1H <- - convertDerivStackNonNegative(param, theta, X, Z, le, lee, leee, d1b, deriv = 1)$d1H
  d1H + exp(r) * S # Valid only because of single smoothing parameter implemented
}

#### TEST OF DERIVATIVES

# gradient
GR <- data.frame(EX = gr(beta), 
                 FD = as.numeric(jacobian(func = ll, x = as.vector(beta))))
GR$DIFF <- apply(GR, 1, function(x) round(diff(x), 7))
with(GR, plot(EX, FD))
abline(0, 1, col = "red")
GR

# hessian
HEX <- data.frame(EX = as.numeric(hes(beta)),
                  FD = as.numeric(jacobian(func = gr, x = as.vector(beta))))
HEX$DIFF <- apply(HEX, 1, function(x) round(diff(x), 7))
with(HEX, plot(EX, FD))
abline(0, 1, col = "red")
HEX

# d beta / d rho
r <- 0
D1B <- data.frame(EX = d3r(r)$d1br, 
                  FD = jacobian(function(x) d3r(x)$betaHat, r))
D1B$DIFF <- apply(D1B, 1, function(x) round(diff(x), 2)) # less precise because of optim
with(D1B, plot(EX, FD))
abline(0, 1, col = "red")
D1B

# hessian wrt rho
# THERE ARE SOME PROBLEMS I THINK DUE TO IDENTIFIABILITY ISSUES
# TRUST ONLY IF NEGATIVE HESSIAN IS POSITIVE DEFINITE
D1H <- data.frame(EX = as.numeric(d1h(r)), 
                  FD = jacobian(function(x) d3r(x)$negHessian, r))
D1H$DIFF <- apply(D1H, 1, function(x) round(diff(x), 2))
with(D1H, plot(EX, FD))
abline(0, 1, col = "red")
D1H
```


