---
title: "gamFactory: tool for building GAM models in mgcv"
author: "Matteo Fasiolo"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
    number_sections: true
vignette: >
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteIndexEntry{mgcViz_vignette}
  %\VignetteEncoding{UTF-8}
---

<style>
body {
text-align: justify}
</style>

```{r setup, include=FALSE}
library(knitr)
opts_chunk$set(out.extra='style="display:block; margin: auto"', fig.align="center", tidy=FALSE)
```


# Testing derivatives of response family

As an example I consider the Generalized Pareto Distribution. We first create the family:
```{r 1, message = F}
library(gamFactory)

n <- 1000
pars <- c(rnorm(1, 0, 1), 1e-2 + rexp(1, 1), rexp(1, 1))
obj <- createGPD( )$initialize(n, pars)
```
The parameters have been randomly simulated, and `n=1000` observations are simulated via `createGPD( )$initialize`. Now we compare exact (`EX`) and finite difference (`FD`) derivative of up to order 3 at the fixed parameters: 
```{r 2, message = F}
fdDeriv(obj = derFunWrapper(obj$derObj), 
        param = pars, 
        ord = 1:3)
```

Now we do a more systematic check, by simulating `np` vectors of parameters from `parSim` and checking the derivatives at each:
```{r 3, message = F}
der <- derivCheck(np = 100, 
                  parSim = function(n){ cbind(rnorm(n), 1 + rexp(n, 1), rexp(n, 1)) }, 
                  obj = obj,
                  ord = 1:3, 
                  trans = function(.x){
                    si <- sign(.x)
                    return( si * sqrt(abs(.x)) )
                  }, 
                  n = 100)
```
Plotting the difference between the analytic and finite-difference derivatives is also useful:
```{r 4, message = F}
par(mfrow = c(2, 2))
for(ii in 1:3) { plot(der[[ii]][ , 1] - der[[ii]][ , 2]) }
```

# Testing derivatives of a non-linear effect

Here we consider an effect of the type
$$
\eta = \sum_{j}\alpha_{j}(z_{i})x_{ij}
$$
where 
$$
\alpha_{1}(z)=\frac{1}{1+\sum_{j\neq1}\exp(\nu_{j})}, \;\;\;\;
\alpha_{k}(z)=\frac{\exp(\nu_{k})}{1+\sum_{j\neq1}\exp(\nu_{j})},\;\;\;\text{for}\;\;\;k=2,\dots,K,
$$
so that $\sum_{j}\alpha_{j}(z_{i})=1$. To create such an effect we do
```{r a1, message = F}
library(gamFactory)

K <- 5
alpha0 <- rnorm(K - 1) 
obj <- createStackEffect( )$initialize(d = K)
```
Then we check the first three derivatives at $alpha0$:
```{r a2, message = F}
fdDeriv(obj = derFunWrapper(obj$derObj, dropHessian = TRUE), 
        param = alpha0, 
        ord = 1:3)
```
Looks good. Now we do some more serious testing:
```{r a3, message = F}
der <- derivCheck(np = 100, 
                  parSim = function(n){ matrix(rnorm(n*(K-1)), n, K-1) }, 
                  obj = obj,
                  ord = 1:3, 
                  trans = function(.x){
                    si <- sign(.x)
                    return( si * sqrt(abs(.x)) )
                  }, 
                  d = K)
```
Different way of plotting:
```{r a4, message = F}
par(mfrow = c(2, 2))
for(ii in 1:3) { plot(der[[ii]][ , 1] - der[[ii]][ , 2]) }
```


# Test of derivatives with positive stack effects (SHASH)

NOTE! For testing derivatives of any general family I have written a function. However, some fixes are still required before we can use only that and delete the next part.

```{r}
set.seed(0)
n <- 1e2
p <- 3
K <- 2
P <- p * K

tau <- rnorm(1)
eps <- rnorm(1)
phi <- rnorm(1)
sig <- exp(tau)
del <- exp(phi)
theta <- c(tau, eps, phi)

betaToPars <- function(beta, theta) { # Transform beta to SHASH parameters
  p <- ncol(Z)
  K <- ncol(X)
  nu <- Z %*% matrix(beta, nrow = p, ncol = K)
  a <- exp(nu)
  eta <- rowSums(a * X)
  mu <- eta
  pars <- cbind(mu, theta[1], theta[2], theta[3])
  pars
}

# Derivatives of log-likelihood wrt eta as functions of beta
lEta <- function(param, deriv = 0) { 
  beta <- param[1:nbeta]
  theta <- param[- (1:nbeta)]
  pars <- betaToPars(beta, theta)
  objSH <- logLikSH(y = y)$derObj(param = pars, deriv = deriv)
  
  l <- objSH$d0(SUM = T)
  le <- lt <- let <- ltt <- lee <- leee <- leet <- lett <- lttt <- NULL # default values
  if (deriv > 0) {
    le <- objSH$d1(SUM = F)[[1]]
    lt <- objSH$d1(SUM = T)[][- 1] # 3x1 1st deriv wrt theta
    
    if (deriv > 1) {
      lee <- objSH$d2(SUM = F)[[1]]
      let <- do.call(cbind, objSH$d2(SUM = F)[2:4]) # nx3 2nd mixed deriv wrt eta theta
      ltt <- matrix(NA, 3, 3)          # 3x3 2nd deriv wrt theta
      ltt[lower.tri(ltt, diag=TRUE)] <- objSH$d2(SUM = T)[- (1:4)]
      ltt <- pmax(ltt, t(ltt), na.rm=TRUE)
      if (deriv > 2) {
        leee <- objSH$d3(SUM = F)[[1]]
        #3rd derivatives
        leee <- objSH$d3(SUM = F)[[1]] 
        leet <- do.call(cbind, objSH$d3(SUM = F)[2:4]) 
        lett <- do.call(cbind, objSH$d3(SUM = F)[5:10]) 
        lttt <- objSH$d3(SUM = T)[11:20] # array of 3rd deriv wrt theta
        
        i3 <- trind.generator(3)$i3
        ltttVec <- objSH$d3(SUM = T)[11:20] # 3x3x3 array of 3rd deriv wrt theta
        lttt <- array(NA, dim = c(3, 3, 3))
        for (uu in 1:3) for (vv in 1:3) for (ww in 1:3) {
          lttt[uu, vv, ww] <- ltttVec[i3[uu, vv, ww]]
          ww <- ww + 1
        }
      }
    }
  }
  
  return(list(l = l, le = le, lee = lee, leee = leee,
              lt = lt, let = let, ltt = ltt,
              leet = leet, lett = lett, lttt = lttt))
}

beta <- rnorm(P)
theta <- rep(1, 3)
trueParam <- c(beta, theta)
nbeta <- length(beta)
nth <- 3
Z <- cbind(1, matrix(runif(n * (p - 1)), nrow = n))
X <- matrix(runif(n * K), nrow = n)
pars <- betaToPars(beta, theta)
tau <- theta[1]
eps <- theta[2]
phi <- theta[3]
sig <- exp(tau)
del <- exp(phi)
y <- pars[, 1] + (del * sig) * sinh((1/del) * asinh(qnorm(runif(n))) + (eps/del))

objSH <- logLikSH(y = y)$derObj(param = pars, deriv = 3)

#### FUNCTIONS FOR TESTING DERIVATIVES

# log-likelihood function
ll <- function(param) {
  lEta(param, deriv = 0)$l
}

# gradient function
gr <- function(param) {
  lEtaList <- lEta(param, deriv = 2)
  le <- lEtaList$le
  lt <- lEtaList$lt
  lee <- lEtaList$lee
  let <- lEtaList$let
  ltt <- lEtaList$ltt
  convertDerivStackPositive(param[1:nbeta], X, Z,
                            le, lt, 
                            lee, let, ltt)$lb
} 

# hessian function
hes <- function(param) {
  lEtaList <- lEta(param, deriv = 2)
  le <- lEtaList$le
  lt <- lEtaList$lt
  lee <- lEtaList$lee
  let <- lEtaList$let
  ltt <- lEtaList$ltt
  convertDerivStackPositive(param[1:nbeta], X, Z,
                            le, lt, 
                            lee, let, ltt)$lbb
}

# function optimizing beta given the smoothing parameter
d3r <- function(r) {
  ## We suppose to have 2 smoothing parameters
  ## Note that theta parameters are unpenalized
  nn <- as.integer(nbeta / 2)
  S1 <- S2 <- matrix(0, nrow = length(param), ncol = length(param))
  diag(S1)[1:nn] <- 1
  diag(S2)[(nn + 1):nbeta] <- 1
  lam <- exp(r)
  # Inner step: optimize pen l wrt beta
  beta0 <- trueParam
  optimization <- nlm(function(x) {
    out <- - ll(x) +
      .5 * lam[1] * t(x) %*% S1 %*% x + .5 * lam[2] * t(x) %*% S2 %*% x
    attr(out, "gradient") <- - gr(x) + lam[1] * S1 %*% x + lam[2] * S2 %*% x
    attr(out, "hessian") <- - hes(x) + lam[1] * S1 + lam[2] * S2
    out
  }, beta0, hessian = T, gradtol = 1e-10)
  betaHat <- optimization$estimate # beta hat as function of rho
  negHessian <- optimization$hessian
  invNegH <- solve(optimization$hessian)
  d1br1 <- - lam[1] * invNegH %*% S1 %*% betaHat # derivative of beta hat wrt rho
  d1br2 <- - lam[2] * invNegH %*% S2 %*% betaHat # derivative of beta hat wrt rho
  d1br <- cbind(d1br1, d1br2)
  list(betaHat = betaHat, d1br = d1br, negHessian = negHessian)
}
# derivative of the hessian of the log-likelihood wrt rho
d1h <- function(r) {
  
  d3rObj <- d3r(r)
  param <- d3rObj$betaHat
  d1b <- d3rObj$d1br
  lEtaList <- lEta(param, deriv = 3)
  le <- lEtaList$le
  lt <- lEtaList$lt
  lee <- lEtaList$lee
  let <- lEtaList$let
  ltt <- lEtaList$ltt
  leee <- lEtaList$leee
  leet <- lEtaList$leet
  lett <- lEtaList$lett
  lttt <- lEtaList$lttt
  d1H <- convertDerivStackPositive(param[1:nbeta], X, Z,
                                   le, lt, 
                                   lee, let, ltt, 
                                   leee, leet, lett, lttt, 
                                   d1b, deriv = 2)$d1H
  do.call(cbind, lapply(d1H, as.numeric))
}


#### TEST OF DERIVATIVES

relDiff <- function(x) round(abs(diff(x)) / max(abs(x)), 2)

# gradient
param <- rnorm(P + nth, sd = 1e0)
# param <- rep(0, P + nth)
r <- c(0, 0)
lam <- exp(r)
S <- diag(c(rep(1, P), rep(0, nth)))
GR <- data.frame(EX = gr(param), 
                 FD = as.numeric(jacobian(func = ll, x = as.vector(param))))
GR$DIFF <- apply(GR, 1, relDiff)
with(GR, plot(EX, FD))
abline(0, 1, col = "red")
GR

# hessian
HEX <- data.frame(EX = as.numeric(hes(param)),
                  FD = as.numeric(jacobian(func = gr, x = as.vector(param))))
HEX$DIFF <- apply(HEX, 1, relDiff)
with(HEX, plot(EX, FD))
abline(0, 1, col = "red")
HEX

# d beta / d rho
r <- c(0, 0)
D1B <- data.frame(EX = as.numeric(d3r(r)$d1br), 
                  FD = as.numeric(jacobian(function(x) d3r(x)$betaHat, r)))
D1B$DIFF <- apply(D1B, 1, relDiff)
with(D1B, plot(EX, FD))
abline(0, 1, col = "red")
D1B

# hessian wrt rho
r <- c(0, 0)
D1H <- data.frame(EX = as.numeric(d1h(r)),
                  FD = as.numeric(jacobian(function(x) as.numeric(hes(d3r(x)$betaHat)), r)))
D1H$DIFF <- apply(D1H, 1, relDiff)
with(D1H, plot(EX, FD))
abline(0, 1, col = "red")
D1H
```


# Positive stack effects using mgcv

Single covariate.

```{r}
set.seed(33)
n <- 1e3
K <- 2
z <- 1:n

a1 <- sin(z * 2 * pi / n) / 2.1 + .5
a2 <- cos(z * 2 * pi / n) / 2.1 + .5
a <- cbind(a1, a2)

nu1 <- log(a1)
nu2 <- log(a2)
nu <- cbind(nu1, nu2)

x1 <- rnorm(n)
x2 <- x1 + rnorm(n, sd = 1)

X <- cbind(x1, x2)
mu <- rowSums(X * a)

## NORMAL
logSigma <- 1
y <- rnorm(n, mean = mu, sd = exp(logSigma))

form <- lapply(1:K, function(x) formula("y ~ s(z)"))
mod <- gam(form, family = stackFamily(X, logLikGAU(y)))

## Plot estimated aplhas vs true
nuHat <- mod$fitted.values
aHat <- exp(nuHat)

plot(aHat[, 1], type = "l", ylim = range(c(aHat[, 1], a1)))
lines(a1, col = "red")

plot(aHat[, 2], type = "l", ylim = range(c(aHat[, 2], a2)))
lines(a2, col = "red")
```

Two covariates z1 and z2

```{r}
set.seed(0)
n <- 1e4
K <- 2
z1 <- runif(n)
z2 <- runif(n)

nu1 <- sin(z1 * 2 * pi) - 1 + cos(z2 * 2 * pi) - 1
nu2 <- cos(z1 * 2 * pi) - 1 + sin(z2 * 2 * pi) - 1
s11 <- scale(sin(z1 * 2 * pi), center = TRUE, scale = FALSE)
s12 <- scale(cos(z2 * 2 * pi), center = TRUE, scale = FALSE)
s21 <- scale(cos(z1 * 2 * pi), center = TRUE, scale = FALSE)
s22 <- scale(sin(z2 * 2 * pi), center = TRUE, scale = FALSE)

nu <- cbind(nu1, nu2)
a <- exp(nu)

x1 <- rnorm(n)
x2 <- x1 + rnorm(n, sd = 1)

X <- cbind(x1, x2)
mu <- rowSums(X * a)

## NORMAL
logSigma <- 1
y <- rnorm(n, mean = mu, sd = exp(logSigma))

form <- lapply(1:K, function(x) formula("y ~ s(z1, k = 5) + s(z2, k = 5)"))
mod <- gam(form, family = stackFamily(X, logLikGAU(y)))

rows1 <- sample(1:n, 1e3)
rows1 <- rows1[order(z1[rows1])]
rows2 <- sample(1:n, 1e3)
rows2 <- rows2[order(z2[rows2])]

## plot(mod, select = 1)
## lines(z1[rows1], s11[rows1], col = "red")

## plot(mod, select = 2)
## lines(z2[rows2], s12[rows2], col = "red")

## plot(mod, select = 3)
## lines(z1[rows1], s21[rows1], col = "red")

## plot(mod, select = 4)
## lines(z2[rows2], s22[rows2], col = "red")
```

